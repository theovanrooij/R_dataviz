# add variable for new cases in last 7 days
for (i in 1:nrow(cv_aggregated)) {
if (i==1) { cv_aggregated$new[i] = 0 }
if (i>1) { cv_aggregated$new[i] = cv_aggregated$cases[i] - cv_aggregated$cases[i-1] }
}
# add plotting region
cv_aggregated$region = "Global"
cv_aggregated$date = as.Date(cv_aggregated$date,"%Y-%m-%d")
# assign colours to countries to ensure consistency between plots
cls = rep(c(brewer.pal(8,"Dark2"), brewer.pal(10, "Paired"), brewer.pal(12, "Set3"), brewer.pal(8,"Set2"), brewer.pal(9, "Set1"), brewer.pal(8, "Accent"),  brewer.pal(9, "Pastel1"),  brewer.pal(8, "Pastel2")),4)
cls_names = c(as.character(unique(cv_cases$country)), as.character(unique(cv_cases_continent$continent)), as.character(unique(cv_states$state)),"Global")
country_cols = cls[1:length(cls_names)]
names(country_cols) = cls_names
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
setwd("D:/DÃ©veloppement/R/nCoV_tracker")
# load required packages
if(!require(magrittr)) install.packages("magrittr", repos = "http://cran.us.r-project.org")
if(!require(rvest)) install.packages("rvest", repos = "http://cran.us.r-project.org")
if(!require(readxl)) install.packages("readxl", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(maps)) install.packages("maps", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(reshape2)) install.packages("reshape2", repos = "http://cran.us.r-project.org")
if(!require(ggiraph)) install.packages("ggiraph", repos = "http://cran.us.r-project.org")
if(!require(RColorBrewer)) install.packages("RColorBrewer", repos = "http://cran.us.r-project.org")
if(!require(leaflet)) install.packages("leaflet", repos = "http://cran.us.r-project.org")
if(!require(plotly)) install.packages("plotly", repos = "http://cran.us.r-project.org")
if(!require(geojsonio)) install.packages("geojsonio", repos = "http://cran.us.r-project.org")
if(!require(shiny)) install.packages("shiny", repos = "http://cran.us.r-project.org")
if(!require(shinyWidgets)) install.packages("shinyWidgets", repos = "http://cran.us.r-project.org")
if(!require(shinydashboard)) install.packages("shinydashboard", repos = "http://cran.us.r-project.org")
if(!require(shinythemes)) install.packages("shinythemes", repos = "http://cran.us.r-project.org")
# set mapping colour for each outbreak
covid_col = "#cc4c02"
covid_other_col = "#662506"
sars_col = "#045a8d"
h1n1_col = "#4d004b"
ebola_col = "#016c59"
# import data
cv_cases = read.csv("input_data/coronavirus.csv")
sars_cases = read.csv("input_data/sars.csv")
countries = read.csv("input_data/countries_codes_and_coordinates.csv")
ebola_cases = read.csv("input_data/ebola.csv")
h1n1_cases = read.csv("input_data/h1n1.csv")
worldcountry = geojson_read("input_data/50m.geojson", what = "sp")
country_geoms = read.csv("input_data/country_geoms.csv")
cv_states = read.csv("input_data/coronavirus_states.csv")
names(sars_cases)[1] = "country"
# extract time stamp from cv_cases
update = tail(cv_cases$last_update,1)
update
# check consistency of country names across datasets
if (all(unique(cv_cases$country) %in% unique(countries$country))==FALSE) { print("Error: inconsistent country names")}
# extract dates from cv data
if (any(grepl("/", cv_cases$date))) {
cv_cases$date = format(as.Date(cv_cases$date, format="%d/%m/%Y"),"%Y-%m-%d")
} else { cv_cases$date = as.Date(cv_cases$date, format="%Y-%m-%d") }
cv_cases$date = as.Date(cv_cases$date)
cv_cases
cv_cases$date
# import data
cv_cases = read.csv("input_data/coronavirus.csv")
cv_cases$date
cv_cases$date = as.Date(cv_cases$date)
## COVID-2019 interactive mapping tool
## Edward Parker, London School of Hygiene & Tropical Medicine (edward.parker@lshtm.ac.uk), last updated April 2020
## includes code adapted from the following sources:
# https://github.com/rstudio/shiny-examples/blob/master/087-crandash/
# https://rviews.rstudio.com/2019/10/09/building-interactive-world-maps-in-shiny/
# https://github.com/rstudio/shiny-examples/tree/master/063-superzip-example
# update data with automated script
#source("jhu_data_daily_cases.R") # option to update daily cases
##  source("jhu_data_weekly_cases.R",local=T) # run locally to update numbers, but not live on Rstudio server /Users/epp11/Dropbox (VERG)/GitHub/nCoV_tracker/app.R(to avoid possible errors on auto-updates)
##  source("ny_data_us.R",local=T) # run locally to update numbers, but not live on Rstudio server (to avoid possible errors on auto-updates)
# load required packages
if(!require(magrittr)) install.packages("magrittr", repos = "http://cran.us.r-project.org")
if(!require(rvest)) install.packages("rvest", repos = "http://cran.us.r-project.org")
if(!require(readxl)) install.packages("readxl", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(maps)) install.packages("maps", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(reshape2)) install.packages("reshape2", repos = "http://cran.us.r-project.org")
if(!require(ggiraph)) install.packages("ggiraph", repos = "http://cran.us.r-project.org")
if(!require(RColorBrewer)) install.packages("RColorBrewer", repos = "http://cran.us.r-project.org")
if(!require(leaflet)) install.packages("leaflet", repos = "http://cran.us.r-project.org")
if(!require(plotly)) install.packages("plotly", repos = "http://cran.us.r-project.org")
if(!require(geojsonio)) install.packages("geojsonio", repos = "http://cran.us.r-project.org")
if(!require(shiny)) install.packages("shiny", repos = "http://cran.us.r-project.org")
if(!require(shinyWidgets)) install.packages("shinyWidgets", repos = "http://cran.us.r-project.org")
if(!require(shinydashboard)) install.packages("shinydashboard", repos = "http://cran.us.r-project.org")
if(!require(shinythemes)) install.packages("shinythemes", repos = "http://cran.us.r-project.org")
# set mapping colour for each outbreak
covid_col = "#cc4c02"
covid_other_col = "#662506"
sars_col = "#045a8d"
h1n1_col = "#4d004b"
ebola_col = "#016c59"
# import data
cv_cases = read.csv("input_data/coronavirus.csv")
sars_cases = read.csv("input_data/sars.csv")
countries = read.csv("input_data/countries_codes_and_coordinates.csv")
ebola_cases = read.csv("input_data/ebola.csv")
h1n1_cases = read.csv("input_data/h1n1.csv")
worldcountry = geojson_read("input_data/50m.geojson", what = "sp")
country_geoms = read.csv("input_data/country_geoms.csv")
cv_states = read.csv("input_data/coronavirus_states.csv")
names(sars_cases)[1] = "country"
### DATA PROCESSING: COVID-19 ###
# extract time stamp from cv_cases
update = tail(cv_cases$last_update,1)
# check consistency of country names across datasets
if (all(unique(cv_cases$country) %in% unique(countries$country))==FALSE) { print("Error: inconsistent country names")}
# extract dates from cv data
if (any(grepl("/", cv_cases$date))) {
cv_cases$date = format(as.Date(cv_cases$date, format="%d/%m/%Y"),"%Y-%m-%d")
} else { cv_cases$date = as.Date(cv_cases$date, format="%Y-%m-%d") }
cv_cases$date = as.Date(cv_cases$date)
cv_min_date = as.Date(min(cv_cases$date),"%Y-%m-%d")
current_date = as.Date(max(cv_cases$date),"%Y-%m-%d")
cv_max_date_clean = format(as.POSIXct(current_date),"%d %B %Y")
# merge cv data with country data and extract key summary variables
cv_cases = merge(cv_cases, countries, by = "country")
cv_cases = cv_cases[order(cv_cases$date),]
cv_cases$cases_per_million = as.numeric(format(round(cv_cases$cases/(cv_cases$population/1000000),1),nsmall=1))
cv_cases$new_cases_per_million = as.numeric(format(round(cv_cases$new_cases/(cv_cases$population/1000000),1),nsmall=1))
cv_cases$million_pop = as.numeric(cv_cases$population>1e6)
cv_cases$deaths_per_million = as.numeric(format(round(cv_cases$deaths/(cv_cases$population/1000000),1),nsmall=1))
cv_cases$new_deaths_per_million = as.numeric(format(round(cv_cases$new_deaths/(cv_cases$population/1000000),1),nsmall=1))
# add variable for weeks since 100th case and 10th death
cv_cases$weeks_since_case100 = cv_cases$weeks_since_death10 = 0
for (i in 1:length(unique(cv_cases$country))) {
country_name = as.character(unique(cv_cases$country))[i]
country_db = subset(cv_cases, country==country_name)
country_db$weeks_since_case100[country_db$cases>=100] = 0:(sum(country_db$cases>=100)-1)
country_db$weeks_since_death10[country_db$deaths>=10] = 0:(sum(country_db$deaths>=10)-1)
cv_cases$weeks_since_case100[cv_cases$country==country_name] = country_db$weeks_since_case100
cv_cases$weeks_since_death10[cv_cases$country==country_name] = country_db$weeks_since_death10
}
# creat variable for today's data
cv_today = subset(cv_cases, date==current_date)
current_case_count = sum(cv_today$cases)
current_case_count_China = sum(cv_today$cases[cv_today$country=="Mainland China"])
current_case_count_other = sum(cv_today$cases[cv_today$country!="Mainland China"])
current_death_count = sum(cv_today$deaths)
# create subset of state data for today's data
if (any(grepl("/", cv_states$date))) {
cv_states$date = format(as.Date(cv_states$date, format="%d/%m/%Y"),"%Y-%m-%d")
} else { cv_states$date = as.Date(cv_states$date, format="%Y-%m-%d") }
cv_states_today = subset(cv_states, date==max(cv_states$date))
# create subset for countries with at least 1000 cases
cv_today_reduced = subset(cv_today, cases>=1000)
# write current day's data
write.csv(cv_today %>% select(c(country, date, update, cases, new_cases, deaths, new_deaths,
cases_per_million, new_cases_per_million,
deaths_per_million, new_deaths_per_million,
weeks_since_case100, weeks_since_death10)), "input_data/coronavirus_today.csv")
# aggregate at continent level
cv_cases_continent = subset(cv_cases, !is.na(continent_level)) %>% select(c(cases, new_cases, deaths, new_deaths, date, continent_level)) %>% group_by(continent_level, date) %>% summarise_each(funs(sum)) %>% data.frame()
# add variable for weeks since 100th case and 10th death
cv_cases_continent$weeks_since_case100 = cv_cases_continent$weeks_since_death10 = 0
cv_cases_continent$continent = cv_cases_continent$continent_level
for (i in 1:length(unique(cv_cases_continent$continent))) {
continent_name = as.character(unique(cv_cases_continent$continent))[i]
continent_db = subset(cv_cases_continent, continent==continent_name)
continent_db$weeks_since_case100[continent_db$cases>=100] = 0:(sum(continent_db$cases>=100)-1)
continent_db$weeks_since_death10[continent_db$deaths>=10] = 0:(sum(continent_db$deaths>=10)-1)
cv_cases_continent$weeks_since_case100[cv_cases_continent$continent==continent_name] = continent_db$weeks_since_case100
cv_cases_continent$weeks_since_death10[cv_cases_continent$continent==continent_name] = continent_db$weeks_since_death10
}
# add continent populations
cv_cases_continent$pop = NA
cv_cases_continent$pop[cv_cases_continent$continent=="Africa"] = 1.2e9
cv_cases_continent$pop[cv_cases_continent$continent=="Asia"] = 4.5e9
cv_cases_continent$pop[cv_cases_continent$continent=="Europe"] = 7.4e8
cv_cases_continent$pop[cv_cases_continent$continent=="North America"] = 5.8e8
cv_cases_continent$pop[cv_cases_continent$continent=="Oceania"] = 3.8e7
cv_cases_continent$pop[cv_cases_continent$continent=="South America"] = 4.2e8
# add normalised counts
cv_cases_continent$cases_per_million =  as.numeric(format(round(cv_cases_continent$cases/(cv_cases_continent$pop/1000000),1),nsmall=1))
cv_cases_continent$new_cases_per_million =  as.numeric(format(round(cv_cases_continent$new_cases/(cv_cases_continent$pop/1000000),1),nsmall=1))
cv_cases_continent$deaths_per_million =  as.numeric(format(round(cv_cases_continent$deaths/(cv_cases_continent$pop/1000000),1),nsmall=1))
cv_cases_continent$new_deaths_per_million =  as.numeric(format(round(cv_cases_continent$new_deaths/(cv_cases_continent$pop/1000000),1),nsmall=1))
write.csv(cv_cases_continent, "input_data/coronavirus_continent.csv")
# aggregate at global level
cv_cases_global = cv_cases %>% select(c(cases, new_cases, deaths, new_deaths, date, global_level)) %>% group_by(global_level, date) %>% summarise_each(funs(sum)) %>% data.frame()
cv_cases_global$weeks_since_case100 = cv_cases_global$weeks_since_death10 = 0:(nrow(cv_cases_global)-1)
# add normalised counts
cv_cases_global$pop = 7.6e9
cv_cases_global$cases_per_million =  as.numeric(format(round(cv_cases_global$cases/(cv_cases_global$pop/1000000),1),nsmall=1))
cv_cases_global$new_cases_per_million =  as.numeric(format(round(cv_cases_global$new_cases/(cv_cases_global$pop/1000000),1),nsmall=1))
cv_cases_global$deaths_per_million =  as.numeric(format(round(cv_cases_global$deaths/(cv_cases_global$pop/1000000),1),nsmall=1))
cv_cases_global$new_deaths_per_million =  as.numeric(format(round(cv_cases_global$new_deaths/(cv_cases_global$pop/1000000),1),nsmall=1))
write.csv(cv_cases_global, "input_data/coronavirus_global.csv")
cv_large_countries
# select large countries for mapping polygons
cv_large_countries = cv_today %>% filter(alpha3 %in% worldcountry$ADM0_A3)
cv_large_countries
cv_large_countrie[1]
cv_large_countries[1]
cv_large_countries[1,]
cv_today
cv_large_countries
cv_today
cv_today[1,]
## COVID-2019 interactive mapping tool
## Edward Parker, London School of Hygiene & Tropical Medicine (edward.parker@lshtm.ac.uk), last updated April 2020
## includes code adapted from the following sources:
# https://github.com/rstudio/shiny-examples/blob/master/087-crandash/
# https://rviews.rstudio.com/2019/10/09/building-interactive-world-maps-in-shiny/
# https://github.com/rstudio/shiny-examples/tree/master/063-superzip-example
# update data with automated script
#source("jhu_data_daily_cases.R") # option to update daily cases
##  source("jhu_data_weekly_cases.R",local=T) # run locally to update numbers, but not live on Rstudio server /Users/epp11/Dropbox (VERG)/GitHub/nCoV_tracker/app.R(to avoid possible errors on auto-updates)
##  source("ny_data_us.R",local=T) # run locally to update numbers, but not live on Rstudio server (to avoid possible errors on auto-updates)
# load required packages
if(!require(magrittr)) install.packages("magrittr", repos = "http://cran.us.r-project.org")
if(!require(rvest)) install.packages("rvest", repos = "http://cran.us.r-project.org")
if(!require(readxl)) install.packages("readxl", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(maps)) install.packages("maps", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(reshape2)) install.packages("reshape2", repos = "http://cran.us.r-project.org")
if(!require(ggiraph)) install.packages("ggiraph", repos = "http://cran.us.r-project.org")
if(!require(RColorBrewer)) install.packages("RColorBrewer", repos = "http://cran.us.r-project.org")
if(!require(leaflet)) install.packages("leaflet", repos = "http://cran.us.r-project.org")
if(!require(plotly)) install.packages("plotly", repos = "http://cran.us.r-project.org")
if(!require(geojsonio)) install.packages("geojsonio", repos = "http://cran.us.r-project.org")
if(!require(shiny)) install.packages("shiny", repos = "http://cran.us.r-project.org")
if(!require(shinyWidgets)) install.packages("shinyWidgets", repos = "http://cran.us.r-project.org")
if(!require(shinydashboard)) install.packages("shinydashboard", repos = "http://cran.us.r-project.org")
if(!require(shinythemes)) install.packages("shinythemes", repos = "http://cran.us.r-project.org")
# set mapping colour for each outbreak
covid_col = "#cc4c02"
covid_other_col = "#662506"
sars_col = "#045a8d"
h1n1_col = "#4d004b"
ebola_col = "#016c59"
# import data
cv_cases = read.csv("input_data/coronavirus.csv")
sars_cases = read.csv("input_data/sars.csv")
countries = read.csv("input_data/countries_codes_and_coordinates.csv")
ebola_cases = read.csv("input_data/ebola.csv")
h1n1_cases = read.csv("input_data/h1n1.csv")
worldcountry = geojson_read("input_data/50m.geojson", what = "sp")
country_geoms = read.csv("input_data/country_geoms.csv")
cv_states = read.csv("input_data/coronavirus_states.csv")
names(sars_cases)[1] = "country"
### DATA PROCESSING: COVID-19 ###
# extract time stamp from cv_cases
update = tail(cv_cases$last_update,1)
# check consistency of country names across datasets
if (all(unique(cv_cases$country) %in% unique(countries$country))==FALSE) { print("Error: inconsistent country names")}
# extract dates from cv data
if (any(grepl("/", cv_cases$date))) {
cv_cases$date = format(as.Date(cv_cases$date, format="%d/%m/%Y"),"%Y-%m-%d")
} else { cv_cases$date = as.Date(cv_cases$date, format="%Y-%m-%d") }
cv_cases$date = as.Date(cv_cases$date)
cv_min_date = as.Date(min(cv_cases$date),"%Y-%m-%d")
current_date = as.Date(max(cv_cases$date),"%Y-%m-%d")
cv_max_date_clean = format(as.POSIXct(current_date),"%d %B %Y")
# merge cv data with country data and extract key summary variables
cv_cases = merge(cv_cases, countries, by = "country")
cv_cases = cv_cases[order(cv_cases$date),]
cv_cases$cases_per_million = as.numeric(format(round(cv_cases$cases/(cv_cases$population/1000000),1),nsmall=1))
cv_cases$new_cases_per_million = as.numeric(format(round(cv_cases$new_cases/(cv_cases$population/1000000),1),nsmall=1))
cv_cases$million_pop = as.numeric(cv_cases$population>1e6)
cv_cases$deaths_per_million = as.numeric(format(round(cv_cases$deaths/(cv_cases$population/1000000),1),nsmall=1))
cv_cases$new_deaths_per_million = as.numeric(format(round(cv_cases$new_deaths/(cv_cases$population/1000000),1),nsmall=1))
# add variable for weeks since 100th case and 10th death
cv_cases$weeks_since_case100 = cv_cases$weeks_since_death10 = 0
for (i in 1:length(unique(cv_cases$country))) {
country_name = as.character(unique(cv_cases$country))[i]
country_db = subset(cv_cases, country==country_name)
country_db$weeks_since_case100[country_db$cases>=100] = 0:(sum(country_db$cases>=100)-1)
country_db$weeks_since_death10[country_db$deaths>=10] = 0:(sum(country_db$deaths>=10)-1)
cv_cases$weeks_since_case100[cv_cases$country==country_name] = country_db$weeks_since_case100
cv_cases$weeks_since_death10[cv_cases$country==country_name] = country_db$weeks_since_death10
}
# creat variable for today's data
cv_today = subset(cv_cases, date==current_date)
current_case_count = sum(cv_today$cases)
current_case_count_China = sum(cv_today$cases[cv_today$country=="Mainland China"])
current_case_count_other = sum(cv_today$cases[cv_today$country!="Mainland China"])
current_death_count = sum(cv_today$deaths)
# create subset of state data for today's data
if (any(grepl("/", cv_states$date))) {
cv_states$date = format(as.Date(cv_states$date, format="%d/%m/%Y"),"%Y-%m-%d")
} else { cv_states$date = as.Date(cv_states$date, format="%Y-%m-%d") }
cv_states_today = subset(cv_states, date==max(cv_states$date))
# create subset for countries with at least 1000 cases
cv_today_reduced = subset(cv_today, cases>=1000)
# write current day's data
write.csv(cv_today %>% select(c(country, date, update, cases, new_cases, deaths, new_deaths,
cases_per_million, new_cases_per_million,
deaths_per_million, new_deaths_per_million,
weeks_since_case100, weeks_since_death10)), "input_data/coronavirus_today.csv")
# aggregate at continent level
cv_cases_continent = subset(cv_cases, !is.na(continent_level)) %>% select(c(cases, new_cases, deaths, new_deaths, date, continent_level)) %>% group_by(continent_level, date) %>% summarise_each(funs(sum)) %>% data.frame()
# add variable for weeks since 100th case and 10th death
cv_cases_continent$weeks_since_case100 = cv_cases_continent$weeks_since_death10 = 0
cv_cases_continent$continent = cv_cases_continent$continent_level
for (i in 1:length(unique(cv_cases_continent$continent))) {
continent_name = as.character(unique(cv_cases_continent$continent))[i]
continent_db = subset(cv_cases_continent, continent==continent_name)
continent_db$weeks_since_case100[continent_db$cases>=100] = 0:(sum(continent_db$cases>=100)-1)
continent_db$weeks_since_death10[continent_db$deaths>=10] = 0:(sum(continent_db$deaths>=10)-1)
cv_cases_continent$weeks_since_case100[cv_cases_continent$continent==continent_name] = continent_db$weeks_since_case100
cv_cases_continent$weeks_since_death10[cv_cases_continent$continent==continent_name] = continent_db$weeks_since_death10
}
# add continent populations
cv_cases_continent$pop = NA
cv_cases_continent$pop[cv_cases_continent$continent=="Africa"] = 1.2e9
cv_cases_continent$pop[cv_cases_continent$continent=="Asia"] = 4.5e9
cv_cases_continent$pop[cv_cases_continent$continent=="Europe"] = 7.4e8
cv_cases_continent$pop[cv_cases_continent$continent=="North America"] = 5.8e8
cv_cases_continent$pop[cv_cases_continent$continent=="Oceania"] = 3.8e7
cv_cases_continent$pop[cv_cases_continent$continent=="South America"] = 4.2e8
# add normalised counts
cv_cases_continent$cases_per_million =  as.numeric(format(round(cv_cases_continent$cases/(cv_cases_continent$pop/1000000),1),nsmall=1))
cv_cases_continent$new_cases_per_million =  as.numeric(format(round(cv_cases_continent$new_cases/(cv_cases_continent$pop/1000000),1),nsmall=1))
cv_cases_continent$deaths_per_million =  as.numeric(format(round(cv_cases_continent$deaths/(cv_cases_continent$pop/1000000),1),nsmall=1))
cv_cases_continent$new_deaths_per_million =  as.numeric(format(round(cv_cases_continent$new_deaths/(cv_cases_continent$pop/1000000),1),nsmall=1))
write.csv(cv_cases_continent, "input_data/coronavirus_continent.csv")
# aggregate at global level
cv_cases_global = cv_cases %>% select(c(cases, new_cases, deaths, new_deaths, date, global_level)) %>% group_by(global_level, date) %>% summarise_each(funs(sum)) %>% data.frame()
cv_cases_global$weeks_since_case100 = cv_cases_global$weeks_since_death10 = 0:(nrow(cv_cases_global)-1)
# add normalised counts
cv_cases_global$pop = 7.6e9
cv_cases_global$cases_per_million =  as.numeric(format(round(cv_cases_global$cases/(cv_cases_global$pop/1000000),1),nsmall=1))
cv_cases_global$new_cases_per_million =  as.numeric(format(round(cv_cases_global$new_cases/(cv_cases_global$pop/1000000),1),nsmall=1))
cv_cases_global$deaths_per_million =  as.numeric(format(round(cv_cases_global$deaths/(cv_cases_global$pop/1000000),1),nsmall=1))
cv_cases_global$new_deaths_per_million =  as.numeric(format(round(cv_cases_global$new_deaths/(cv_cases_global$pop/1000000),1),nsmall=1))
write.csv(cv_cases_global, "input_data/coronavirus_global.csv")
# select large countries for mapping polygons
cv_large_countries = cv_today %>% filter(alpha3 %in% worldcountry$ADM0_A3)
if (all(cv_large_countries$alpha3 %in% worldcountry$ADM0_A3)==FALSE) { print("Error: inconsistent country names")}
cv_large_countries = cv_large_countries[order(cv_large_countries$alpha3),]
# create plotting parameters for map
bins = c(0,10,50,100,500,1000,Inf)
cv_pal <- colorBin("Reds", domain = cv_large_countries$cases_per_million, bins = bins)
plot_map <- worldcountry[worldcountry$ADM0_A3 %in% cv_large_countries$alpha3, ]
# creat cv base map
basemap = leaflet(plot_map) %>%
addTiles() %>%
addLayersControl(
position = "bottomright",
overlayGroups = c("2019-COVID (new)", "2019-COVID (cumulative)", "2003-SARS", "2009-H1N1 (swine flu)", "2014-Ebola"),
options = layersControlOptions(collapsed = FALSE)) %>%
hideGroup(c("2019-COVID (cumulative)", "2003-SARS", "2009-H1N1 (swine flu)", "2014-Ebola")) %>%
addProviderTiles(providers$CartoDB.Positron) %>%
fitBounds(~-100,-60,~60,70) %>%
addLegend("bottomright", pal = cv_pal, values = ~cv_large_countries$deaths_per_million,
title = "<small>Deaths per million</small>")
basemap
runApp()
# creat cv base map
basemap = leaflet(plot_map) %>%
addTiles() %>%
addLayersControl(
options = layersControlOptions(collapsed = FALSE)) %>%
addProviderTiles(providers$CartoDB.Positron) %>%
fitBounds(~-100,-60,~60,70) %>%
addLegend("bottomright", pal = cv_pal, values = ~starlader$nbRep,
title = "<small>ReprÃ©sentant par pays</small>") %>%
addPolygons(stroke = FALSE, smoothFactor = 1, fillOpacity = 0.5, fillColor = ~cv_pal(starlader$nbRep))
library(lubridate)
library(lubridate)
library(tidyr)
library(tidyverse)
df <- as_tibble(read.csv("data/players.csv"))
names(df)
results <- as_tibble(read.csv("data/results.csv"))
median(results$rank_1)
# Pas tous les ids de chaque table. Dans result, on retrouve des tournois peu populaire et pas dans players
diff <-setdiff(id_df,id_result)
length(diff)
teams_rank <- data.frame(results %>% group_by(match_id, team_1, team_2, rank_1, rank_2) %>% summarise())
pivot <- pivot_longer(teams_rank, !c("match_id"), names_to = c(".value"), names_pattern= "(.)")
df <- merge(df,pivot,by.x=c("match_id","team"),by.y=c("match_id","t"))
df <- as_tibble(merge(df,pivot,by.x=c("match_id","opponent"),by.y=c("match_id","t")))
rating_moyen_joueur_annÃ©e_nbMatch <- df %>% group_by(year(date),player_id,player_name) %>% summarise(nbMatch = n(),mean_rating = mean(rating,na.rm = TRUE)) %>% arrange(desc(nbMatch))
rating_moyen_joueur_annÃ©e_nbMatch %>% filter(player_name == "ZywOo")
names(rating_moyen_joueur_annÃ©e_nbMatch)[1]<- "AnnÃ©e"
rating_moyen_joueur_annÃ©e_nbMatch
p <- ggplot(rating_moyen_joueur_annÃ©e_nbMatch %>% filter(AnnÃ©e == "2018" && nbMatch > 70), aes(x=nbMatch,y=mean_rating))
p <- p + geom_point()
p
# NÃ©cessaire ID car plusieurs pseudo similaire (ALEX par exemple)
nb_match_joueur_annee <- df %>% group_by(year(date),player_name,player_id) %>% summarise(nbMatch=n()) %>% arrange(desc(nbMatch))
names(nb_match_joueur_annee)[1] <- "AnnÃ©e"
nb_match_joueur_annee
p <- ggplot(nb_match_joueur_annee %>% filter(player_name == "shox"), aes(x=AnnÃ©e,y=nbMatch))
p <- p + geom_line()
p
# Division par 10 car 10 observations par Ã©quipe (5 joueurs par Ã©quipe)
nb_match_tournoi <- df %>% group_by(event_name) %>% summarise(nbObservation=n()/10) %>% arrange(desc(nbObservation))
nb_match_tournoi
rating_joueur_tournoi <- df %>% group_by(event_name,event_id,player_name,player_id) %>% summarise(mean_rating = mean(rating)) %>% arrange(desc(mean_rating))
rating_joueur_tournoi %>% filter(event_name == "ESEA MDL Season 30 Europe")
rating_pays <- df %>% group_by(country) %>% summarise(mean_rating = mean(rating)) %>% arrange(desc(mean_rating))
rating_pays
if(!require(geojsonio)) install.packages("geojsonio", repos = "http://cran.us.r-project.org")
if(!require(leaflet)) install.packages("leaflet", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
# On isole les joueurs par tounois puis pay pays pour avopir le nombre de reprÃ©sentant d chaque pays par tournoi
pays_tournois <- df %>% group_by(event_id,event_name,player_id,player_name,country) %>% summarise() %>% group_by(event_name,event_id,country) %>% summarise(nbRep = n())
starlader = pays_tournois %>% filter(event_id == 4443)
starlader[starlader == "United Kingdom"] <- "UK"
starlader[starlader == "United States"] <- "USA"
countries = read.csv("data/countries_codes_and_coordinates.csv")
names(countries)
starlader =merge(starlader,countries,by="country")
worldcountry = geojson_read("data/50m.geojson", what = "sp")
starlader = starlader[order(starlader$alpha3),]
# create plotting parameters for map
bins = c(1,3,5,7,9,11,13,15,Inf)
cv_pal <- colorBin("Oranges", domain = starlader$nbRep,bins = bins)
plot_map <- worldcountry[worldcountry$ADM0_A3 %in% starlader$alpha3, ]
# creat cv base map
basemap = leaflet(plot_map) %>%
addTiles() %>%
addLayersControl(
options = layersControlOptions(collapsed = FALSE)) %>%
addProviderTiles(providers$CartoDB.Positron) %>%
fitBounds(~-100,-60,~60,70) %>%
addLegend("bottomright", pal = cv_pal, values = ~starlader$nbRep,
title = "<small>ReprÃ©sentant par pays</small>") %>%
addPolygons(stroke = FALSE, smoothFactor = 1, fillOpacity = 0.5, fillColor = ~cv_pal(starlader$nbRep))
basemap
if(!require(shiny)) install.packages("shiny", repos = "http://cran.us.r-project.org")
names(df)
results <- as_tibble(read.csv("data/results.csv"))
median(results$rank_1)
# Pas tous les ids de chaque table. Dans result, on retrouve des tournois peu populaire et pas dans players
diff <-setdiff(id_df,id_result)
length(diff)
teams_rank <- data.frame(results %>% group_by(match_id, team_1, team_2, rank_1, rank_2) %>% summarise())
results <- as_tibble(read.csv("data/results.csv"))
median(results$rank_1)
teams_rank <- data.frame(results %>% group_by(match_id, team_1, team_2, rank_1, rank_2) %>% summarise())
pivot <- pivot_longer(teams_rank, !c("match_id"), names_to = c(".value"), names_pattern= "(.)")
df <- merge(df,pivot,by.x=c("match_id","team"),by.y=c("match_id","t"))
df <- as_tibble(merge(df,pivot,by.x=c("match_id","opponent"),by.y=c("match_id","t")))
rating_moyen_joueur_annÃ©e_nbMatch <- df %>% group_by(year(date),player_id,player_name) %>% summarise(nbMatch = n(),mean_rating = mean(rating,na.rm = TRUE)) %>% arrange(desc(nbMatch))
rating_moyen_joueur_annÃ©e_nbMatch %>% filter(player_name == "ZywOo")
names(rating_moyen_joueur_annÃ©e_nbMatch)[1]<- "AnnÃ©e"
rating_moyen_joueur_annÃ©e_nbMatch
p <- ggplot(rating_moyen_joueur_annÃ©e_nbMatch %>% filter(AnnÃ©e == "2018" && nbMatch > 70), aes(x=nbMatch,y=mean_rating))
p <- p + geom_point()
p
# NÃ©cessaire ID car plusieurs pseudo similaire (ALEX par exemple)
nb_match_joueur_annee <- df %>% group_by(year(date),player_name,player_id) %>% summarise(nbMatch=n()) %>% arrange(desc(nbMatch))
names(nb_match_joueur_annee)[1] <- "AnnÃ©e"
nb_match_joueur_annee
p <- ggplot(nb_match_joueur_annee %>% filter(player_name == "shox"), aes(x=AnnÃ©e,y=nbMatch))
p <- p + geom_line()
p
# Division par 10 car 10 observations par Ã©quipe (5 joueurs par Ã©quipe)
nb_match_tournoi <- df %>% group_by(event_name) %>% summarise(nbObservation=n()/10) %>% arrange(desc(nbObservation))
nb_match_tournoi
rating_joueur_tournoi <- df %>% group_by(event_name,event_id,player_name,player_id) %>% summarise(mean_rating = mean(rating)) %>% arrange(desc(mean_rating))
rating_joueur_tournoi %>% filter(event_name == "ESEA MDL Season 30 Europe")
rating_pays <- df %>% group_by(country) %>% summarise(mean_rating = mean(rating)) %>% arrange(desc(mean_rating))
rating_pays
if(!require(geojsonio)) install.packages("geojsonio", repos = "http://cran.us.r-project.org")
if(!require(leaflet)) install.packages("leaflet", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
# On isole les joueurs par tounois puis pay pays pour avopir le nombre de reprÃ©sentant d chaque pays par tournoi
pays_tournois <- df %>% group_by(event_id,event_name,player_id,player_name,country) %>% summarise() %>% group_by(event_name,event_id,country) %>% summarise(nbRep = n())
starlader = pays_tournois %>% filter(event_id == 4443)
starlader[starlader == "United Kingdom"] <- "UK"
starlader[starlader == "United States"] <- "USA"
countries = read.csv("data/countries_codes_and_coordinates.csv")
names(countries)
starlader =merge(starlader,countries,by="country")
worldcountry = geojson_read("data/50m.geojson", what = "sp")
starlader = starlader[order(starlader$alpha3),]
# create plotting parameters for map
bins = c(1,3,5,7,9,11,13,15,Inf)
cv_pal <- colorBin("Oranges", domain = starlader$nbRep,bins = bins)
plot_map <- worldcountry[worldcountry$ADM0_A3 %in% starlader$alpha3, ]
# creat cv base map
basemap = leaflet(plot_map) %>%
addTiles() %>%
addLayersControl(
options = layersControlOptions(collapsed = FALSE)) %>%
addProviderTiles(providers$CartoDB.Positron) %>%
fitBounds(~-100,-60,~60,70) %>%
addLegend("bottomright", pal = cv_pal, values = ~starlader$nbRep,
title = "<small>ReprÃ©sentant par pays</small>") %>%
addPolygons(stroke = FALSE, smoothFactor = 1, fillOpacity = 0.5, fillColor = ~cv_pal(starlader$nbRep))
basemap
if(!require(shiny)) install.packages("shiny", repos = "http://cran.us.r-project.org")
library(shiny)
# Define UI for application that draws a histogram
shinyUI(fluidPage(
# Application title
titlePanel("Old Faithful Geyser Data"),
))
# Define server logic required to draw a histogram
shinyServer(function(input, output) {
})
shinyApp(ui, server)
